import cv2
import time
import os
from collections import deque
from PyQt6.QtCore import QThread, pyqtSignal
from ultralytics import YOLO
import numpy as np

# Thi·∫øt l·∫≠p th∆∞ m·ª•c g·ªëc ƒë·ªÉ l∆∞u d·ªØ li·ªáu
ROOT_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
DATA_DIR = os.path.join(ROOT_DIR, "data")
if not os.path.exists(DATA_DIR):
    os.makedirs(DATA_DIR)

class DetectionThread(QThread):
    """
    Thread x·ª≠ l√Ω ph√°t hi·ªán s·ª± c·ªë giao th√¥ng b·∫±ng YOLO
    Ch·∫°y trong background ƒë·ªÉ kh√¥ng l√†m ƒë∆° UI
    """
    change_pixmap_signal = pyqtSignal(np.ndarray)  # Signal ph√°t frame ƒë√£ v·∫Ω ƒë·ªÉ hi·ªÉn th·ªã
    detection_signal = pyqtSignal(str, str)  # Signal ph√°t khi ph√°t hi·ªán s·ª± c·ªë (label, image_path)
    snapshot_saved = pyqtSignal(str, str, str)  # Signal ph√°t 3 ƒë∆∞·ªùng d·∫´n ·∫£nh (tr∆∞·ªõc, trong, sau)
    process_finished_signal = pyqtSignal(dict)  # Signal ph√°t khi ho√†n th√†nh x·ª≠ l√Ω (cho ch·∫ø ƒë·ªô analyst)
    progress_signal = pyqtSignal(int)  # Signal ph√°t ti·∫øn ƒë·ªô x·ª≠ l√Ω (ph·∫ßn trƒÉm)

    def __init__(self, model_path='best.pt', source=0, save_path=None, custom_labels="accident, vehicle accident", conf_threshold=0.70, loop=True):
        """
        Kh·ªüi t·∫°o thread ph√°t hi·ªán
        
        Args:
            model_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file m√¥ h√¨nh YOLO
            source: Ngu·ªìn video (0 = webcam, ho·∫∑c ƒë∆∞·ªùng d·∫´n file)
            save_path: ƒê∆∞·ªùng d·∫´n l∆∞u video ƒë√£ x·ª≠ l√Ω (None = kh√¥ng l∆∞u)
            custom_labels: C√°c nh√£n c·∫ßn ph√°t hi·ªán, ph√¢n c√°ch b·ªüi d·∫•u ph·∫©y
            conf_threshold: Ng∆∞·ª°ng ƒë·ªô tin c·∫≠y (0.0 - 1.0)
            loop: C√≥ l·∫∑p l·∫°i video kh√¥ng (True = l·∫∑p, False = ch·∫°y m·ªôt l·∫ßn)
        """
        super().__init__()
        self.model_path = model_path
        self.source = source
        self.save_path = save_path
        self.custom_labels = custom_labels
        self.conf_threshold = conf_threshold
        self.loop = loop  # ƒêi·ªÅu khi·ªÉn h√†nh vi l·∫∑p l·∫°i
        self.model = None
        self.running = True
        self.paused = False
        self.out = None

    def pause(self):
        """
        Chuy·ªÉn ƒë·ªïi gi·ªØa t·∫°m d·ª´ng v√† ti·∫øp t·ª•c
        Tr·∫£ v·ªÅ tr·∫°ng th√°i m·ªõi (True = ƒëang t·∫°m d·ª´ng)
        """
        self.paused = not self.paused
        return self.paused

    def run(self):
        """
        H√†m ch√≠nh ch·∫°y trong thread
        X·ª≠ l√Ω video frame-by-frame, ph√°t hi·ªán s·ª± c·ªë v√† ch·ª•p ·∫£nh
        """
        # Ph√¢n t√≠ch c√°c nh√£n c·∫ßn ph√°t hi·ªán t·ª´ chu·ªói custom_labels
        target_labels = [l.strip().lower() for l in self.custom_labels.split(',') if l.strip()]
        
        # 1. T·∫£i m√¥ h√¨nh YOLO
        try:
            print(f"Loading model from {self.model_path}...")
            self.model = YOLO(self.model_path)
        except Exception as e:
            print(f"Error loading model: {e}")
            return

        # 2. M·ªü ngu·ªìn video (webcam ho·∫∑c file)
        cap = cv2.VideoCapture(self.source)
        if not cap.isOpened():
            print("Cannot open video source")
            return

        # --- C·∫§U H√åNH TH·ªúI GIAN ƒê·ªòNG ---
        # L·∫•y th√¥ng tin FPS v√† t·ªïng s·ªë frame c·ªßa video
        video_fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        # N·∫øu kh√¥ng c√≥ FPS ho·∫∑c FPS = 0, d√πng gi√° tr·ªã m·∫∑c ƒë·ªãnh
        if video_fps == 0 or np.isnan(video_fps): 
            video_fps = 30  # Gi√° tr·ªã d·ª± ph√≤ng
        
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        # --- T√≠nh to√°n k√≠ch th∆∞·ªõc resize tr∆∞·ªõc ---
        # Gi·∫£m k√≠ch th∆∞·ªõc frame l·ªõn ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô x·ª≠ l√Ω
        target_width = width
        target_height = height
        if width > 640:
            scale = 640 / width
            target_width = 640
            target_height = int(height * scale)

        # Thi·∫øt l·∫≠p Video Writer ƒë·ªÉ l∆∞u video ƒë√£ x·ª≠ l√Ω
        if self.save_path:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            self.out = cv2.VideoWriter(self.save_path, fourcc, video_fps, (target_width, target_height))
        
        # C·∫•u h√¨nh th·ªùi gian ch·ª•p ·∫£nh (kh·ªõp v·ªõi server.py)
        BEFORE_SECONDS = 4.0  # Ch·ª•p ·∫£nh "tr∆∞·ªõc" c√°ch 4 gi√¢y
        AFTER_SECONDS = 5.0   # Ch·ª•p ·∫£nh "sau" c√°ch 5 gi√¢y
        
        # T√≠nh to√°n k√≠ch th∆∞·ªõc buffer v√† s·ªë frame c·∫ßn thi·∫øt
        BUFFER_SIZE = int(video_fps * BEFORE_SECONDS)  # Buffer ch·ª©a 4 gi√¢y frame
        AFTER_FRAMES_REQUIRED = int(video_fps * AFTER_SECONDS)  # S·ªë frame c·∫ßn ƒë·ª£i ƒë·ªÉ ch·ª•p "sau"
        
        SKIP_FRAMES = 3  # X·ª≠ l√Ω m·ªói frame th·ª© 3 ƒë·ªÉ tƒÉng t·ªëc (kh·ªõp v·ªõi server)

        
        # Buffer l∆∞u tr·ªØ c√°c frame g·∫ßn ƒë√¢y (d√πng deque ƒë·ªÉ t·ª± ƒë·ªông x√≥a frame c≈©)
        frame_buffer = deque(maxlen=BUFFER_SIZE)
        
        # C√°c bi·∫øn tr·∫°ng th√°i
        snapshot_state = "IDLE"  # Tr·∫°ng th√°i: IDLE, WAITING_FOR_AFTER
        frames_since_incident = 0  # S·ªë frame ƒë√£ tr√¥i qua k·ªÉ t·ª´ khi ph√°t hi·ªán s·ª± c·ªë
        current_incident_label = ""  # Nh√£n c·ªßa s·ª± c·ªë hi·ªán t·∫°i
        current_sequence_id = 0  # ID c·ªßa chu·ªói ·∫£nh ch·ª•p hi·ªán t·∫°i
        last_alert_time = 0  # Th·ªùi gian c·∫£nh b√°o cu·ªëi c√πng
        alert_cooldown = 30  # Th·ªùi gian ch·ªù gi·ªØa c√°c c·∫£nh b√°o (gi√¢y)
        current_accident_streak = 0  # ƒê·∫øm s·ªë frame li√™n ti·∫øp ph√°t hi·ªán s·ª± c·ªë
        
        # Theo d√µi d·ª± ph√≤ng (fallback) - l∆∞u ph√°t hi·ªán t·ªët nh·∫•t n·∫øu kh√¥ng c√≥ s·ª± c·ªë k√©o d√†i
        best_fallback_conf = 0.0  # ƒê·ªô tin c·∫≠y t·ªët nh·∫•t
        best_fallback_data = None  # (label, frame_before, frame_during)
        
        frame_count = 0  # ƒê·∫øm s·ªë frame ƒë√£ x·ª≠ l√Ω
        last_boxes = []  # L∆∞u k·∫øt qu·∫£ detection c·ªßa frame tr∆∞·ªõc ƒë·ªÉ t√°i s·ª≠ d·ª•ng
        
        # Logic ch·ªëng nh·∫•p nh√°y (Anti-Flicker)
        # Cho ph√©p m·ªôt s·ªë frame kh√¥ng ph√°t hi·ªán m√† kh√¥ng reset streak
        missing_frame_tolerance_count = 0
        MAX_MISSING_FRAMES = 5  # Cho ph√©p 5 frame (kho·∫£ng 0.15s) kh√¥ng ph√°t hi·ªán
        
        # L∆∞u frame khi s·ª± c·ªë b·∫Øt ƒë·∫ßu
        potential_incident_frame = None
        
        # Theo d√µi s·ª± c·ªë cu·ªëi c√πng ƒë·ªÉ t·∫°o b√°o c√°o cu·ªëi
        final_snapshots = []
        final_incident_id = None

        print(f"Video Info: FPS={video_fps}, Buffer Size={BUFFER_SIZE}, After Frames={AFTER_FRAMES_REQUIRED}")

        # 3. V√íNG L·∫∂P CH√çNH
        self.running = True
        while self.running and cap.isOpened():
            # --- LOGIC T·∫†M D·ª™NG ---
            if self.paused:
                time.sleep(0.1)  # Ng·ªß ƒë·ªÉ ti·∫øt ki·ªám CPU
                continue
                
            # --- LOGIC L·∫∂P L·∫†I & ƒê·ªåC FRAME ---
            ret, frame = cap.read()
            
            # T·ª± ƒë·ªông l·∫∑p l·∫°i n·∫øu video k·∫øt th√∫c V√Ä ch·∫ø ƒë·ªô l·∫∑p B·∫¨T
            if not ret:
                if self.loop:
                    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                    ret, frame = cap.read()
                    if not ret: break 
                else:
                    # Ch·∫ø ƒë·ªô kh√¥ng l·∫∑p: Ch·ªâ d·ª´ng l·∫°i
                    print("End of video stream (No Loop).")
                    break
            
            # --- T·ªêI ∆ØU H√ìA (Resize ƒë·ªÉ tƒÉng t·ªëc) ---
            # Gi·∫£m k√≠ch th∆∞·ªõc frame l·ªõn xu·ªëng t·ªëi ƒëa 640px chi·ªÅu r·ªông ƒë·ªÉ c·∫£i thi·ªán FPS
            h, w = frame.shape[:2]
            if w > 640:
                scale = 640 / w
                new_w, new_h = 640, int(h * scale)
                frame = cv2.resize(frame, (new_w, new_h))

            last_valid_frame = frame.copy()  # L∆∞u frame h·ª£p l·ªá cu·ªëi c√πng
            frame_buffer.append(frame.copy())  # Th√™m v√†o buffer
            frame_count += 1
            annotated_frame = frame.copy()  # Frame ƒë·ªÉ v·∫Ω annotation
            
            # Ph√°t signal ti·∫øn ƒë·ªô (vi·ªác l·∫∑p l√†m ph·ª©c t·∫°p, nh∆∞ng c√≥ th·ªÉ wrap)
            if total_frames > 0:
                # Wrap ti·∫øn ƒë·ªô 0-100% m·ªói l·∫ßn l·∫∑p
                current_loop_frame = frame_count % total_frames
                progress = int((current_loop_frame / total_frames) * 100)
                self.progress_signal.emit(progress)

            # --- A. PH√ÅT HI·ªÜN ---
            # Logic b·ªè qua frame (Server d√πng % 3)
            if frame_count % 3 == 0:
                # Ch·∫°y YOLO ƒë·ªÉ ph√°t hi·ªán v√† theo d√µi ƒë·ªëi t∆∞·ª£ng
                results = self.model.track(frame, persist=True, verbose=False, conf=self.conf_threshold)

                
                last_boxes = []  # Reset danh s√°ch box ƒë·ªÉ l∆∞u k·∫øt qu·∫£ m·ªõi
                
                current_time = time.time()
                is_incident_now = False
                detected_label = ""

                # Duy·ªát qua t·∫•t c·∫£ k·∫øt qu·∫£ ph√°t hi·ªán
                for result in results:
                    for box in result.boxes:
                        x1, y1, x2, y2 = map(int, box.xyxy[0])  # T·ªça ƒë·ªô bounding box
                        cls_id = int(box.cls[0])  # ID l·ªõp
                        label = self.model.names[cls_id]  # T√™n l·ªõp
                        conf = float(box.conf[0])  # ƒê·ªô tin c·∫≠y
                        
                        last_boxes.append((x1, y1, x2, y2, label, conf))
                        
                        # Ki·ªÉm tra xem label c√≥ trong danh s√°ch c·∫ßn ph√°t hi·ªán kh√¥ng
                        if label.lower() in target_labels:
                            detected_label = label
                            
                            # C·∫≠p nh·∫≠t ·ª©ng vi√™n d·ª± ph√≤ng (ƒë·ªô tin c·∫≠y t·ªët nh·∫•t)
                            # D√πng khi kh√¥ng c√≥ s·ª± c·ªë k√©o d√†i nh∆∞ng c√≥ ph√°t hi·ªán t·ªët
                            if conf > best_fallback_conf:
                                best_fallback_conf = conf
                                fb_before = frame_buffer[0].copy() if frame_buffer else frame.copy()
                                fb_during = frame.copy()
                                best_fallback_data = (label, fb_before, fb_during)

                # --- LOGIC X√ÅC NH·∫¨N (Ki·ªÉm tra ƒë·ªô b·ªÅn v·ªØng) ---
                # C·∫ßn ph√°t hi·ªán li√™n ti·∫øp trong m·ªôt kho·∫£ng th·ªùi gian ƒë·ªÉ x√°c nh·∫≠n s·ª± c·ªë
                ACCIDENT_DURATION_THRESHOLD = 0.5  # C·∫ßn 0.5 gi√¢y ƒë·ªÉ x√°c th·ª±c
                CONFIRMATION_FRAMES = int(video_fps * ACCIDENT_DURATION_THRESHOLD)
                
                if detected_label:
                    # C√≥ ph√°t hi·ªán s·ª± c·ªë trong frame n√†y
                    current_accident_streak += 1  # TƒÉng streak
                    missing_frame_tolerance_count = 0  # Reset tolerance khi ph√°t hi·ªán
                    
                    # Ch·ª•p kho·∫£nh kh·∫Øc ch√≠nh x√°c khi s·ª± c·ªë B·∫ÆT ƒê·∫¶U (Streak == 1)
                    if current_accident_streak == 1:
                        # B√ô ƒê·∫ÆP CHO ƒê·ªò TR·ªÑ C·ª¶A AI
                        # AI ph√°t hi·ªán m·∫•t v√†i frame, ng∆∞·ªùi d√πng c·∫£m th·∫•y n√≥ "mu·ªôn"
                        # L·∫•y frame t·ª´ ~0.3 gi√¢y TR∆Ø·ªöC t·ª´ buffer ƒë·ªÉ l·∫•y "Kho·∫£nh kh·∫Øc va ch·∫°m"
                        rewind_frames = int(video_fps * 0.3)  # Tua ng∆∞·ª£c 0.3 gi√¢y
                        if len(frame_buffer) > rewind_frames:
                            potential_incident_frame = frame_buffer[-rewind_frames].copy()
                        elif frame_buffer:
                            potential_incident_frame = frame_buffer[0].copy()
                        else:
                            potential_incident_frame = frame.copy()
                        
                else:
                    # KH√îNG c√≥ ph√°t hi·ªán trong frame n√†y
                    # CH·ªêNG NH·∫§P NH√ÅY: Kh√¥ng reset ngay l·∫≠p t·ª©c
                    if current_accident_streak > 0 and missing_frame_tolerance_count < MAX_MISSING_FRAMES:
                        missing_frame_tolerance_count += 1
                        # Duy tr√¨ streak (kh√¥ng tƒÉng, kh√¥ng reset)
                    else:
                        # Reset streak n·∫øu ƒë√£ v∆∞·ª£t qu√° tolerance
                        current_accident_streak = 0
                        potential_incident_frame = None
                        missing_frame_tolerance_count = 0

                # --- K√çCH HO·∫†T S·ª∞ KI·ªÜN ---
                # ƒêi·ªÅu ki·ªán ƒë·ªÉ b·∫Øt ƒë·∫ßu ch·ª•p ·∫£nh:
                # 1. ƒêang ·ªü tr·∫°ng th√°i IDLE (ch∆∞a ch·ª•p)
                # 2. ƒê√£ qua th·ªùi gian cooldown gi·ªØa c√°c c·∫£nh b√°o
                # 3. Streak ƒë√£ ƒë·∫°t ng∆∞·ª°ng x√°c nh·∫≠n
                if snapshot_state == "IDLE" and \
                   (current_time - last_alert_time > alert_cooldown) and \
                   current_accident_streak >= CONFIRMATION_FRAMES:
                    
                    is_incident_now = True
                    
                    # Logic tua ng∆∞·ª£c ƒë·ªÉ l·∫•y frame "During" t·ªët h∆°n
                    SECONDS_TO_REWIND = 1.0  # Tua ng∆∞·ª£c 1 gi√¢y
                    frames_back = int(video_fps * SECONDS_TO_REWIND)
                    
                    # L·∫•y frame t·ª´ buffer (∆∞u ti√™n frame c≈© h∆°n ƒë·ªÉ b·∫Øt kho·∫£nh kh·∫Øc va ch·∫°m)
                    if len(frame_buffer) > frames_back:
                        snap_frame = frame_buffer[-frames_back].copy()  # L·∫•y ·∫£nh t·ª´ 1 gi√¢y tr∆∞·ªõc
                        print(f"üì∏ Captured frame from {SECONDS_TO_REWIND}s ago!")
                    elif frame_buffer:
                        snap_frame = frame_buffer[0].copy()  # L·∫•y ·∫£nh c≈© nh·∫•t c√≥ th·ªÉ
                    else:
                        snap_frame = frame.copy()  # B·∫•t ƒë·∫Øc dƒ© m·ªõi l·∫•y ·∫£nh hi·ªán t·∫°i

                    # C·∫≠p nh·∫≠t c√°c bi·∫øn tr·∫°ng th√°i
                    last_alert_time = current_time
                    current_sequence_id = int(time.time())  # ID duy nh·∫•t cho chu·ªói ·∫£nh n√†y
                    final_incident_id = current_sequence_id
                    current_incident_label = detected_label
                    frames_since_incident = 0

                    # --- L∆ØU ·∫¢NH ---
                    if self.loop:
                         # Ch·∫ø ƒë·ªô Live: L∆∞u Before v√† During ngay
                         # 1. Before: L·∫•y t·ª´ ƒë·∫ßu buffer (frame c≈© nh·∫•t)
                         frame_before = frame_buffer[0] if frame_buffer else frame
                         path_before = self.save_image(frame_before, current_sequence_id, detected_label, "1_before")

                         # 2. During: L∆∞u frame ƒë√£ tua ng∆∞·ª£c (kho·∫£nh kh·∫Øc va ch·∫°m)
                         path_during = self.save_image(snap_frame, current_sequence_id, detected_label, "2_during")
                        
                         current_snapshot_paths = [path_before, path_during, None]
                         final_snapshots = current_snapshot_paths
                        
                         # Ph√°t signal ƒë·ªÉ UI c·∫≠p nh·∫≠t
                         self.snapshot_saved.emit(*current_snapshot_paths)
                        
                         snapshot_state = "WAITING_FOR_AFTER"  # Chuy·ªÉn sang ch·ªù ·∫£nh "After"
                        
                    else:
                        # Ch·∫ø ƒë·ªô Analyst (kh√¥ng l·∫∑p)
                        snapshot_state = "WAITING_FOR_AFTER"
                        
                        # 1. Before: L·∫•y ·∫£nh c≈© nh·∫•t trong buffer (c√°ch ƒë√¢y 4s)
                        frame_before = frame_buffer[0] if frame_buffer else frame
                        path_before = self.save_image(frame_before, current_sequence_id, detected_label, "1_before")
                        
                        # 2. During: L·∫•y frame ƒë√£ tua ng∆∞·ª£c
                        path_during = self.save_image(snap_frame, current_sequence_id, detected_label, "2_during")
                        
                        current_snapshot_paths = [path_before, path_during, None]
                        final_snapshots = current_snapshot_paths 
                        
                        # Ph√°t signal ph√°t hi·ªán
                        self.detection_signal.emit(detected_label, path_during)

            # --- B. V·∫º BOXES & TIMESTAMP ---
            # Th√™m timestamp v√†o frame (theo style c·ªßa server)
            time_str = str(time.strftime("%H:%M:%S", time.gmtime(frame_count / video_fps)))
            # V·∫Ω outline ƒëen tr∆∞·ªõc, sau ƒë√≥ v·∫Ω text v√†ng ƒë·ªÉ d·ªÖ ƒë·ªçc
            cv2.putText(annotated_frame, f"Time: {time_str}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 4)
            cv2.putText(annotated_frame, f"Time: {time_str}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)

            # V·∫Ω c√°c bounding box v√† nh√£n
            for (x1, y1, x2, y2, label, conf) in last_boxes:
                # M√†u ƒë·ªè cho s·ª± c·ªë, m√†u xanh cho ƒë·ªëi t∆∞·ª£ng kh√°c
                color = (0, 0, 255) if label.lower() in target_labels else (0, 255, 0)
                # V·∫Ω h√¨nh ch·ªØ nh·∫≠t
                cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)
                
                # V·∫Ω nh√£n v·ªõi n·ªÅn
                text = f"{label} {conf:.2f}"
                (w, h), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)
                cv2.rectangle(annotated_frame, (x1, y1 - 20), (x1 + w, y1), color, -1)
                cv2.putText(annotated_frame, text, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

            
            # --- THANH DEBUG ---
            # Hi·ªÉn th·ªã thanh ti·∫øn ƒë·ªô x√°c nh·∫≠n khi ƒëang x√°c nh·∫≠n s·ª± c·ªë
            if current_accident_streak > 0 and snapshot_state == "IDLE":
                bar_width = min(int((current_accident_streak / CONFIRMATION_FRAMES) * 100), 100)
                cv2.rectangle(annotated_frame, (10, 10), (10 + bar_width, 20), (0, 0, 255), -1)

            # --- C. C·∫¨P NH·∫¨T STATE MACHINE ---
            # X·ª≠ l√Ω logic ch·ªù ch·ª•p ·∫£nh "After"
            if snapshot_state == "WAITING_FOR_AFTER":
                frames_since_incident += 1
                if frames_since_incident >= AFTER_FRAMES_REQUIRED:
                    # 3. L∆∞u ·∫£nh AFTER (sau khi ƒë√£ ƒë·ª£i ƒë·ªß s·ªë frame)
                    path_after = self.save_image(frame, current_sequence_id, current_incident_label, "3_after")
                    print("Sequence capture complete.")
                    
                    if 'current_snapshot_paths' in locals():
                        current_snapshot_paths[2] = path_after
                        self.snapshot_saved.emit(*current_snapshot_paths)
                        final_snapshots = current_snapshot_paths 

                    snapshot_state = "IDLE"  # Quay v·ªÅ tr·∫°ng th√°i ch·ªù

            # --- D. ƒê·∫¶U RA ---
            # Ph√°t frame ƒë√£ v·∫Ω ƒë·ªÉ UI hi·ªÉn th·ªã
            self.change_pixmap_signal.emit(annotated_frame)
            # Ghi v√†o video n·∫øu c√≥
            if self.out:
                self.out.write(annotated_frame)

        # D·ªçn d·∫πp
        print("Stopping detection thread...")
        cap.release()
        if self.out:
            self.out.release()
            
        # B·∫ÆT BU·ªòC HO√ÄN TH√ÄNH SNAPSHOT N·∫æU ƒêANG CH·ªú
        # N·∫øu video k·∫øt th√∫c tr∆∞·ªõc khi ch·ª•p ƒë∆∞·ª£c ·∫£nh "After"
        if snapshot_state == "WAITING_FOR_AFTER" and 'current_snapshot_paths' in locals():
            print("Video ended before 'After' frame. Saving last frame as 'After'.")
            frame_after = last_valid_frame if last_valid_frame is not None else frame
            if frame_after is not None:
                path_after = self.save_image(frame_after, current_sequence_id, current_incident_label, "3_after")
                current_snapshot_paths[2] = path_after
                final_snapshots = current_snapshot_paths
        
        # --- LOGIC D·ª∞ PH√íNG M·ªöI ---
        # N·∫øu kh√¥ng c√≥ snapshot n√†o ƒë∆∞·ª£c t·∫°o, nh∆∞ng c√≥ ph√°t hi·ªán g√¨ ƒë√≥
        # D√πng d·ªØ li·ªáu d·ª± ph√≤ng t·ªët nh·∫•t ƒë·ªÉ t·∫°o snapshot
        if not final_snapshots and best_fallback_data is not None:
            print(f"‚ö†Ô∏è No prolonged incident confirmed. Using FALLBACK snapshot (Best Conf: {best_fallback_conf:.2f})")
            fb_label, fb_before, fb_during = best_fallback_data
            fb_seq_id = int(time.time())
            
            p1 = self.save_image(fb_before, fb_seq_id, fb_label, "1_before")
            p2 = self.save_image(fb_during, fb_seq_id, fb_label, "2_during")
            # D√πng frame cu·ªëi l√†m 'After'
            last_frame = last_valid_frame if last_valid_frame is not None else fb_during
            p3 = self.save_image(last_frame, fb_seq_id, fb_label, "3_after")
            
            final_snapshots = [p1, p2, p3]
            final_incident_id = fb_seq_id
            
            # Ph√°t signal ƒë·ªÉ UI c·∫≠p nh·∫≠t
            self.detection_signal.emit(fb_label, p2)

        
        # Ph√°t signal ho√†n th√†nh (cho ch·∫ø ƒë·ªô analyst)
        self.process_finished_signal.emit({
            'success': True,
            'output_path': self.save_path,
            'snapshots': final_snapshots,
            'incident_id': str(final_incident_id) if final_incident_id else str(int(time.time()))
        })
            
    def stop(self):
        """
        G·ª≠i t√≠n hi·ªáu d·ª´ng thread v√† ƒë·ª£i n√≥ k·∫øt th√∫c
        """
        self.running = False
        self.wait()

    def save_image(self, frame, seq_id, label, suffix):
        """
        L∆∞u ·∫£nh v√†o th∆∞ m·ª•c data
        
        Args:
            frame: Frame c·∫ßn l∆∞u
            seq_id: ID c·ªßa chu·ªói ·∫£nh
            label: Nh√£n c·ªßa s·ª± c·ªë
            suffix: H·∫≠u t·ªë (1_before, 2_during, 3_after)
        
        Returns:
            ƒê∆∞·ªùng d·∫´n file ƒë√£ l∆∞u
        """
        if not os.path.exists(DATA_DIR):
            os.makedirs(DATA_DIR)
        
        filename = f"{seq_id}_{label}_{suffix}.jpg"
        filepath = os.path.join(DATA_DIR, filename)
        cv2.imwrite(filepath, frame)
        return filepath